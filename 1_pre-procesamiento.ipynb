{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importación de librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pltp\n",
    "import spacy\n",
    "import nltk\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Cargar el modelo para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")  # Modelo pequeño en español\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('recursos/dataset_prueba.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b) Pre procesamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Preprocesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesa:\n",
    "    def __init__(self):\n",
    "        self.text=''\n",
    "\n",
    "    import string\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        # Definir todos los signos de puntuación y caracteres especiales\n",
    "        forbidden = {\n",
    "            \"?\", \"¿\", \"¡\", \"!\", \"'\", '\"', \"‘\", \"’\", \"“\", \"”\", \"<\", \">\", \"(\", \")\", \n",
    "            \".\", \",\", \":\", \";\", \"-\", \"&\", \"@\", \"/\", \"N/A\", \"#\", \"$\", \"´\", \"`\", \"“\", \"”\",\n",
    "            \"“\", \"”\", \"‘\", \"’\",\"” \",\"“\", \"«\", \"»\", \"—\", \"–\", \"…\", \"•\"\n",
    "        }\n",
    "        \n",
    "        # Combinar los signos de puntuación de Python con los caracteres prohibidos\n",
    "        all_punctuation = set(string.punctuation).union(forbidden)\n",
    "        \n",
    "        # Eliminar todos los signos de puntuación y caracteres especiales\n",
    "        punctuationfree = \"\".join([char for char in text if char not in all_punctuation])\n",
    "        \n",
    "        # Reemplazar múltiples espacios en blanco por un solo espacio\n",
    "        punctuationfree = \" \".join(punctuationfree.split())\n",
    "        \n",
    "        return punctuationfree.strip()\n",
    "        \n",
    "            \n",
    "    def lower_words(self,text):\n",
    "        words_lower = text.lower()\n",
    "        return words_lower\n",
    "    \n",
    "\n",
    "\n",
    "    def quitarAcentos(self,s):\n",
    "        replacements = (\n",
    "            (\"á\", \"a\"),\n",
    "            (\"é\", \"e\"),\n",
    "            (\"í\", \"i\"),\n",
    "            (\"ó\", \"o\"),\n",
    "            (\"ú\", \"u\"),\n",
    "            )\n",
    "        for a, b in replacements:\n",
    "            s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "        return s\n",
    "    \n",
    "    def remove_stopwords(self,text):\n",
    "        stop_words = [\"y\", \"en\", \"de\", \"para\", \"con\", \"el\", \"la\", \"los\", \"las\", \"un\", \"una\", \"unos\", \"unas\", \n",
    "        \"por\", \"que\", \"a\", \"o\", \"e\",\"su\",\"sus\",\n",
    "            \"a\", \"actualmente\", \"adelante\", \"además\", \"afirmó\", \"agregó\", \"ahora\", \"ahí\", \"al\", \"algo\",\n",
    "        \"alguna\", \"algunas\", \"alguno\", \"algunos\", \"algún\", \"alrededor\", \"ambos\", \"ampleamos\", \"ante\",\n",
    "        \"anterior\", \"antes\", \"apenas\", \"aproximadamente\", \"aquel\", \"aquellas\", \"aquellos\", \"aqui\",\n",
    "        \"aquí\", \"arriba\", \"aseguró\", \"así\", \"atras\", \"aunque\", \"ayer\", \"añadió\", \"aún\", \"bajo\",\n",
    "        \"bastante\", \"bien\", \"buen\", \"buena\", \"buenas\", \"bueno\", \"buenos\", \"cada\", \"casi\", \"cerca\",\n",
    "        \"cierta\", \"ciertas\", \"cierto\", \"ciertos\", \"cinco\", \"comentó\", \"como\", \"con\", \"conocer\",\n",
    "        \"conseguimos\", \"conseguir\", \"considera\", \"consideró\", \"consigo\", \"consigue\", \"consiguen\",\n",
    "        \"consigues\", \"contra\", \"cosas\", \"creo\", \"cual\", \"cuales\", \"cualquier\", \"cuando\", \"cuanto\",\n",
    "        \"cuatro\", \"cuenta\", \"cómo\", \"da\", \"dado\", \"dan\", \"dar\", \"de\", \"debe\", \"deben\", \"debido\",\n",
    "        \"decir\", \"dejó\", \"del\", \"demás\", \"dentro\", \"desde\", \"después\", \"dice\", \"dicen\", \"dicho\",\n",
    "        \"dieron\", \"diferente\", \"diferentes\", \"dijeron\", \"dijo\", \"dio\", \"donde\", \"dos\", \"durante\",\n",
    "        \"e\", \"ejemplo\", \"el\", \"ella\", \"ellas\", \"ello\", \"ellos\", \"embargo\", \"empleais\", \"emplean\",\n",
    "        \"emplear\", \"empleas\", \"empleo\", \"en\", \"encima\", \"encuentra\", \"entonces\", \"entre\", \"era\",\n",
    "        \"erais\", \"eramos\", \"eran\", \"eras\", \"eres\", \"es\", \"esa\", \"esas\", \"ese\", \"eso\", \"esos\", \"esta\",\n",
    "        \"estaba\", \"estabais\", \"estaban\", \"estabas\", \"estad\", \"estada\", \"estadas\", \"estado\", \"estados\",\n",
    "        \"estais\", \"estamos\", \"estan\", \"estando\", \"estar\", \"estaremos\", \"estará\", \"estarán\", \"estarás\",\n",
    "        \"estaré\", \"estaréis\", \"estaría\", \"estaríais\", \"estaríamos\", \"estarían\", \"estarías\", \"estas\",\n",
    "        \"este\", \"estemos\", \"esto\", \"estos\", \"estoy\", \"estuve\", \"estuviera\", \"estuvierais\", \"estuvieran\",\n",
    "        \"estuvieras\", \"estuvieron\", \"estuviese\", \"estuvieseis\", \"estuviesen\", \"estuvieses\", \"estuvimos\",\n",
    "        \"estuviste\", \"estuvisteis\", \"estuviéramos\", \"estuviésemos\", \"estuvo\", \"está\", \"estábamos\",\n",
    "        \"estáis\", \"están\", \"estás\", \"esté\", \"estéis\", \"estén\", \"estés\", \"ex\", \"existe\", \"existen\",\n",
    "        \"explicó\", \"expresó\", \"fin\", \"fue\", \"fuera\", \"fuerais\", \"fueran\", \"fueras\", \"fueron\", \"fuese\",\n",
    "        \"fueseis\", \"fuesen\", \"fueses\", \"fui\", \"fuimos\", \"fuiste\", \"fuisteis\", \"fuéramos\", \"fuésemos\",\n",
    "        \"gran\", \"grandes\", \"gueno\", \"ha\", \"haber\", \"habida\", \"habidas\", \"habido\", \"habidos\", \"habiendo\",\n",
    "        \"habremos\", \"habrá\", \"habrán\", \"habrás\", \"habré\", \"habréis\", \"habría\", \"habríais\", \"habríamos\",\n",
    "        \"habrían\", \"habrías\", \"habéis\", \"había\", \"habíais\", \"habíamos\", \"habían\", \"habías\", \"hace\",\n",
    "        \"haceis\", \"hacemos\", \"hacen\", \"hacer\", \"hacerlo\", \"haces\", \"hacia\", \"haciendo\", \"hago\", \"han\",\n",
    "        \"has\", \"hasta\", \"hay\", \"haya\", \"hayamos\", \"hayan\", \"hayas\", \"hayáis\", \"he\", \"hecho\", \"hemos\",\n",
    "        \"hicieron\", \"hizo\", \"hoy\", \"hube\", \"hubiera\", \"hubierais\", \"hubieran\", \"hubieras\", \"hubieron\",\n",
    "        \"hubiese\", \"hubieseis\", \"hubiesen\", \"hubieses\", \"hubimos\", \"hubiste\", \"hubisteis\", \"hubiéramos\",\n",
    "        \"hubiésemos\", \"hubo\", \"igual\", \"incluso\", \"indicó\", \"informó\", \"intenta\", \"intentais\", \"intentamos\",\n",
    "        \"intentan\", \"intentar\", \"intentas\", \"intento\", \"ir\", \"junto\", \"la\", \"lado\", \"largo\", \"las\", \"le\",\n",
    "        \"les\", \"llegó\", \"lleva\", \"llevar\", \"lo\", \"los\", \"luego\", \"lugar\", \"manera\", \"manifestó\", \"mayor\",\n",
    "        \"me\", \"mediante\", \"mejor\", \"mencionó\", \"menos\", \"mi\", \"mientras\", \"mio\", \"mis\", \"misma\", \"mismas\",\n",
    "        \"mismo\", \"mismos\", \"modo\", \"momento\", \"mucha\", \"muchas\", \"mucho\", \"muchos\", \"muy\", \"más\", \"mí\",\n",
    "        \"mía\", \"mías\", \"mío\", \"míos\", \"nada\", \"nadie\", \"ni\", \"ninguna\", \"ningunas\", \"ninguno\", \"ningunos\",\n",
    "        \"ningún\", \"no\", \"nos\", \"nosotras\", \"nosotros\", \"nuestra\", \"nuestras\", \"nuestro\", \"nuestros\", \"nueva\",\n",
    "        \"nuevas\", \"nuevo\", \"nuevos\", \"nunca\", \"o\", \"ocho\", \"os\", \"otra\", \"otras\", \"otro\", \"otros\", \"para\",\n",
    "        \"parece\", \"parte\", \"partir\", \"pasada\", \"pasado\", \"pero\", \"pesar\", \"poca\", \"pocas\", \"poco\", \"pocos\",\n",
    "        \"podeis\", \"podemos\", \"poder\", \"podria\", \"podriais\", \"podriamos\", \"podrian\", \"podrias\", \"podrá\",\n",
    "        \"podrán\", \"podría\", \"podrían\", \"poner\", \"por\", \"por qué\", \"porque\", \"posible\", \"primer\", \"primera\",\n",
    "        \"primero\", \"primeros\", \"principalmente\", \"propia\", \"propias\", \"propio\", \"propios\", \"próximo\", \"próximos\",\n",
    "        \"pudo\", \"pueda\", \"puede\", \"pueden\", \"puedo\", \"pues\", \"que\", \"quedó\", \"queremos\", \"quien\", \"quienes\",\n",
    "        \"quiere\", \"quién\", \"qué\", \"realizado\", \"realizar\", \"realizó\", \"respecto\", \"sabe\", \"sabeis\", \"sabemos\",\n",
    "        \"saben\", \"saber\", \"sabes\", \"se\", \"sea\", \"seamos\", \"sean\", \"seas\", \"segunda\", \"segundo\", \"según\", \"seis\",\n",
    "        \"ser\", \"seremos\", \"será\", \"serán\", \"serás\", \"seré\", \"seréis\", \"sería\", \"seríais\", \"seríamos\", \"serían\",\n",
    "        \"serías\", \"seáis\", \"señaló\", \"si\", \"sido\", \"siempre\", \"siendo\", \"siete\", \"sigue\", \"siguiente\", \"sin\",\n",
    "        \"sino\", \"sobre\", \"sois\", \"sola\", \"solamente\", \"solas\", \"solo\", \"solos\", \"somos\", \"son\", \"soy\", \"su\",\n",
    "        \"sus\", \"suya\", \"suyas\", \"suyo\", \"suyos\", \"sí\", \"sólo\", \"tal\", \"también\", \"tampoco\", \"tan\", \"tanto\",\n",
    "        \"te\", \"tendremos\", \"tendrá\", \"tendrán\", \"tendrás\", \"tendré\", \"tendréis\", \"tendría\", \"tendríais\",\n",
    "        \"tendríamos\", \"tendrían\", \"tendrías\", \"tened\", \"teneis\", \"tenemos\", \"tener\", \"tenga\", \"tengamos\",\n",
    "        \"tengan\", \"tengas\", \"tengo\", \"tengáis\", \"tenida\", \"tenidas\", \"tenido\", \"tenidos\", \"teniendo\",\n",
    "        \"tenéis\", \"tenía\", \"teníais\", \"teníamos\", \"tenían\", \"tenías\", \"tercera\", \"ti\", \"tiempo\", \"tiene\",\n",
    "        \"tienen\", \"tienes\", \"toda\", \"todas\", \"todavía\", \"todo\", \"todos\", \"total\", \"trabaja\", \"trabajais\",\n",
    "        \"trabajamos\", \"trabajan\", \"trabajar\", \"trabajas\", \"trabajo\", \"tras\", \"trata\", \"través\", \"tres\",\n",
    "        \"tu\", \"tus\", \"tuve\", \"tuviera\", \"tuvierais\", \"tuvieran\", \"tuvieras\", \"tuvieron\", \"tuviese\", \"tuvieseis\",\n",
    "        \"tuviesen\", \"tuvieses\", \"tuvimos\", \"tuviste\", \"tuvisteis\", \"tuviéramos\", \"tuviésemos\", \"tuvo\", \"tuya\",\n",
    "        \"tuyas\", \"tuyo\", \"tuyos\", \"tú\", \"ultimo\", \"un\", \"una\", \"unas\", \"uno\", \"unos\", \"usa\", \"usais\", \"usamos\",\n",
    "        \"usan\", \"usar\", \"usas\", \"uso\", \"usted\", \"va\", \"vais\", \"valor\", \"vamos\", \"van\", \"varias\", \"varios\",\n",
    "        \"vaya\", \"veces\", \"ver\", \"verdad\", \"verdadera\", \"verdadero\", \"vez\", \"vosotras\", \"vosotros\", \"voy\",\n",
    "        \"vuestra\", \"vuestras\", \"vuestro\", \"vuestros\", \"y\", \"ya\", \"yo\", \"él\", \"éramos\", \"ésta\", \"éstas\",\n",
    "        \"éste\", \"éstos\", \"última\", \"últimas\", \"último\", \"últimos\"]\n",
    "        text = str(text)\n",
    "        palabras = text.split()\n",
    "        stop_words_free = [p for p in palabras if p.lower() not in stop_words]\n",
    "        return \" \".join(stop_words_free)\n",
    "\n",
    "\n",
    "    def remove_numbers(self, text):\n",
    "        # Eliminar todos los caracteres numéricos\n",
    "        text_without_numbers = ''.join([char for char in text if not char.isdigit()])\n",
    "        \n",
    "        # Reemplazar múltiples espacios en blanco por un solo espacio\n",
    "        text_without_numbers = \" \".join(text_without_numbers.split())\n",
    "        \n",
    "        return text_without_numbers.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicar métodos de la clase y guardar cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador = Preprocesa()\n",
    "\n",
    "dataset[\"news_limpio\"] = dataset[\"noticia\"].astype(str).apply(preprocesador.remove_stopwords).apply(preprocesador.remove_punctuation).apply(preprocesador.lower_words).apply(preprocesador.remove_numbers).apply(preprocesador.quitarAcentos)\n",
    "\n",
    "# Guardar el dataset procesado \n",
    "dataset.to_csv(\"recursos/dataset_prueba_limpio.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lematizar con SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el diccionario de palabras en español desde el archivo\n",
    "with open(\"recursos/diccionario_limpio.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    valid_words = set(word.strip().lower() for word in f.readlines())\n",
    "\n",
    "lemmas_por_documento = []\n",
    "for text in dataset[\"news_limpio\"]:\n",
    "    doc = nlp(text)\n",
    "    # Guardar solo los lemas que estén en el diccionario y no sean puntuación o espacios\n",
    "    lemmas = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_punct and not token.is_space and token.lemma_.lower() in valid_words\n",
    "    ]\n",
    "    lemmas_por_documento.append(lemmas)\n",
    "\n",
    "\n",
    "# Guardar el dataset procesado \n",
    "dataset.to_csv(\"recursos/dataset_prueba_limpio.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizar y Lematizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmas_por_documento = []\n",
    "for text in dataset[\"news_limpio\"]:\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]  \n",
    "    lemmas_por_documento.append(lemmas)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar lemas en un archivo para su visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'lemas_por_documento.txt' guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "with open(\"archivos/lemas_por_documento.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in lemmas_por_documento:\n",
    "        f.write(f\"{doc}\\n\")\n",
    "\n",
    "print(\"Archivo 'lemas_por_documento.txt' guardado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separar los documentos por clase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('archivos/lemas_por_documento.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "lemas_por_documento = [eval(line.strip()) for line in lines]  # Lista de listas de lemas\n",
    "\n",
    "\n",
    "etiquetas = dataset[\"Type\"].values  # Vector de clases (mismo orden que lemas_por_documento)\n",
    "\n",
    "# Crear diccionario para agrupar lemas por clase\n",
    "lemas_por_clase = {}\n",
    "\n",
    "for lema_doc, clase in zip(lemas_por_documento, etiquetas):\n",
    "    if clase not in lemas_por_clase:\n",
    "        lemas_por_clase[clase] = []\n",
    "    lemas_por_clase[clase].append(lema_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Guardar cada clase en un archivo TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for clase, documentos in lemas_por_clase.items():\n",
    "    nombre_archivo = f\"archivos/lemas_{clase.lower()}.txt\"\n",
    "    with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "        for doc in documentos:\n",
    "            f.write(f\"{doc}\\n\")\n",
    "            #f.write(\" \".join(doc) + \"\\n\")  # Cada documento en una línea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
