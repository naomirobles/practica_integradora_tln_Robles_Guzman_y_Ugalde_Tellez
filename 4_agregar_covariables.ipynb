{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde1282e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ae75de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pltp\n",
    "import spacy\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Cargar el modelo para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")  # Modelo pequeño en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8bf7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_covariables_lemas(lista_lemas):\n",
    "    \"\"\"\n",
    "    Versión modificada que trabaja directamente con listas de lemas\n",
    "    \"\"\"\n",
    "    tokens = [token.lower() for token in lista_lemas]  # Convertir a minúsculas\n",
    "    \n",
    "    with open(\"recursos/palabras_positivas.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        palabras_positivas = set(word.strip().lower() for word in f.readlines())\n",
    "\n",
    "    with open(\"recursos/palabras_negativas.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        palabras_negativas = set(word.strip().lower() for word in f.readlines())\n",
    "\n",
    "    with open(\"recursos/palabras_clave.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        palabras_clave = set(word.strip().lower() for word in f.readlines())\n",
    "\n",
    "    num_pos = sum(token in palabras_positivas for token in tokens)\n",
    "    num_neg = sum(token in palabras_negativas for token in tokens)\n",
    "    \n",
    "    total = len(tokens) if len(tokens) > 0 else 1  # evitar división por 0\n",
    "    \n",
    "    # palabras clave específicas\n",
    "    frecuencia_claves = {}\n",
    "    for palabra in palabras_clave:\n",
    "        frecuencia_claves[f\"freq_{palabra}\"] = tokens.count(palabra)\n",
    "\n",
    "    palabras_alegria = {'feliz', 'contento', 'alegre', 'satisfecho', 'encantado', 'euforico', 'contenta','satisfecha', 'encantada', 'euforica' }\n",
    "    palabras_tristeza = {'triste', 'deprimido', 'melancolico', 'desanimado', 'abatido', 'desalentado', 'pesimista', 'tristeza', 'depresion', 'melancolia', 'desanimo', 'desaliento', 'deprimida', 'melancolica', 'desanimada'}\n",
    "    palabras_enojo = {'enojado', 'furioso', 'irritado', 'molesto', 'indignado', 'enojada', 'furiosa', 'irritada', 'molesta', 'indignada'}\n",
    "    palabras_miedo = {'miedo', 'temor', 'panico', 'terror', 'ansiedad', 'preocupacion'}\n",
    "    \n",
    "    num_alegria = sum(1 for token in tokens if token in palabras_alegria)\n",
    "    num_tristeza = sum(1 for token in tokens if token in palabras_tristeza)\n",
    "    num_enojo = sum(1 for token in tokens if token in palabras_enojo)\n",
    "    num_miedo = sum(1 for token in tokens if token in palabras_miedo)\n",
    "    \n",
    "    covariables = {\n",
    "        \"num_palabras_positivas\": num_pos,\n",
    "        \"num_palabras_negativas\": num_neg,\n",
    "        \"proporcion_palabras_positivas\": num_pos / total,\n",
    "        \"proporcion_palabras_negativas\": num_neg / total,\n",
    "        \"sentimiento_lexico_total\": num_pos - num_neg,\n",
    "        \"num_palabras_alegria\": num_alegria,\n",
    "        \"num_palabras_tristeza\": num_tristeza,\n",
    "        \"num_palabras_enojo\": num_enojo,\n",
    "        \"num_palabras_miedo\": num_miedo,\n",
    "        \"proporcion_alegria\": num_alegria / total,\n",
    "        \"proporcion_tristeza\": num_tristeza / total,\n",
    "        \"proporcion_enojo\": num_enojo / total,\n",
    "        \"proporcion_miedo\": num_miedo / total,\n",
    "    }\n",
    "\n",
    "    # Agregar frecuencias de palabras clave\n",
    "    covariables.update(frecuencia_claves)\n",
    "    \n",
    "    return covariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154583e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment(text):\n",
    "    scores = []\n",
    "    for lemma in text:\n",
    "        if lemma in sentilex:\n",
    "            scores.append(sentilex[lemma])\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        return 0.0\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9e1141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_of_docs(docs):\n",
    "    scores  = []\n",
    "    for doc in docs:\n",
    "        scores.append(score_sentiment(doc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a96507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min, Max, Mean, Std of scores: -0.09930555555555556 0.5707777777777778 0.27393756544904513 0.08824029009832776\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open('archivos/lemas_por_documento.txt', 'r', encoding='utf-8') as f:\n",
    "        listas_de_lemmas = [\n",
    "            ast.literal_eval(line.strip())\n",
    "            for line in f\n",
    "            if line.strip()  # descarta líneas vacías\n",
    "        ]\n",
    "\n",
    "    tree = ET.parse(\"recursos/senticon.es.xml\")\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sentilex = {}\n",
    "    for layer in root.findall(\"layer\"):\n",
    "        for sign in (\"positive\", \"negative\"):\n",
    "            for lemma in layer.find(sign).findall(\"lemma\"):\n",
    "                text = lemma.text.strip()\n",
    "                pol  = float(lemma.get(\"pol\"))\n",
    "                # solo añadimos si no existe, así tomamos la primera capa\n",
    "                if text not in sentilex:\n",
    "                    sentilex[text] = pol\n",
    "                   \n",
    "    scores = score_of_docs(listas_de_lemmas)\n",
    "    print(\"Min, Max, Mean, Std of scores:\", min(scores), max(scores), pd.Series(scores).mean(), pd.Series(scores).std())\n",
    "    \n",
    "    # Aplicar la función a cada fila\n",
    "    lista_covariables = []\n",
    "    for lista_lemas in listas_de_lemmas:\n",
    "        covariables = extraer_covariables_lemas(lista_lemas)\n",
    "        lista_covariables.append(covariables)\n",
    "\n",
    "    lemmas = [' '.join(lemmas) for lemmas in listas_de_lemmas]\n",
    "    \n",
    "    # OPCIÓN 1: Crear un DataFrame combinado desde el inicio\n",
    "    df_combined = pd.DataFrame({\n",
    "        'lemmas': lemmas,\n",
    "        'scores': scores,\n",
    "    })\n",
    "    \n",
    "    # Crear DataFrame de covariables y combinarlo\n",
    "    df_covariables = pd.DataFrame(lista_covariables)\n",
    "    \n",
    "    # Combinar usando concat (por columnas)\n",
    "    df_final = pd.concat([df_combined, df_covariables], axis=1)\n",
    "    \n",
    "    # Guardar todo en un solo CSV\n",
    "    df_final.to_csv('archivos/sentiment_scores_combined.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
